{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYpQgYdxS6ZHcEod0mE8Xh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"ex2.3\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Find a local minima of the function:  ğ‘“(ğ‘¥,ğ‘¦)=sin(ğ‘¥)cos(y) using Gradient Descent"],"metadata":{"id":"_rhai110f_yP"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Find a local minima of the function:  ğ‘“(ğ‘¥,ğ‘¦)=sin(ğ‘¥)cos(y)\n","# Start with random x,y (i.e. random init)\n","# Donâ€™t forget to detach after update and set requires_grad to True\n","\n","x = torch.randn(1, requires_grad=True)\n","y = torch.randn(1, requires_grad=True)\n","lr = 0.1\n","iters = 1000\n","last_z = np.inf\n","# and progress with Gradient Descent\n","\n","for it in range(iters):\n","    z = torch.sin(x) * torch.cos(y)\n","    z.backward()\n","\n","    # if torch.abs(x.grad) < .01 and torch.abs(y.grad) < .01:\n","    #     break\n","\n","    # or\n","\n","    if np.abs(z.item() - last_z) < .01:\n","        break\n","\n","    # update & grad maintain\n","    x = x - lr * x.grad\n","    y = y - lr * y.grad\n","    x.detach()  # Forget how you were created\n","    y.detach()\n","    x.requires_grad_(True)  # calc grad next bw, detach sets you to False\n","    y.requires_grad_(True)\n","    x.retain_grad()  # Donâ€™t delete the grad after it is computed (only needed if the other two are not done at the same time of calculation)\n","    y.retain_grad()\n","    last_z = z.item()\n","\n","print(it, x.item(), y.item(), z.item())\n","\n","# Questions:\n","# What are the resulting parameters (x, y) ? - min arg for z\n","# What is the effect of smaller/larger LR? - larger means less iters to convergence\n","# Run the program multiple times. Does it converge to a different solutions? +- -0.99\n","\n","# %% Validate that the optimum found by GD makes sense\n","xx = [np.random.randn() for i in range(100)]\n","yy = [np.random.randn() for i in range(100)]\n","zz = np.sin(xx) * np.cos(yy)\n","print(zz.min())\n"],"metadata":{"id":"yW4y9P32mEE8"},"execution_count":null,"outputs":[]}]}