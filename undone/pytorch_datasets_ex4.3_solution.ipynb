{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdW9NGocU0TfL6n9J5PUtx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"clean file & gitignore again\"\n","!git config --global user.email \"ronyginosar@mail.huji.ac.il\"\n","!git config --global user.name \"ronyginosar\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Empty"],"metadata":{"id":"LFmDKFJPrSST"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","import json\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","dataset_root = ''\n","\n","# 1. Implement the custom Dataset\n","class CityPersons(Dataset):\n","    def __init__(self, dataset_root, split, ...):\n","        assert split in ['train','val','test']\n","        ...\n","\n","    def __len__(self):\n","        return ...\n","\n","    def __getitem__(self, idx):\n","        ...\n","        return img, seg\n","\n","    # visualize a sample of the dataset (index idx)\n","    def show(self, idx):\n","        ...\n","\n","# 2. Create an instance of the dataset and verify by visualization\n","dataset = CityPersons(...)\n","\n","# 3. Write a custom collate function\n","def my_collate(samples_list):\n","    ...\n","    return batch\n","\n","# 4. Create a DataLoader using the custom collate function\n","dataloader = DataLoader(dataset, ...)\n","\n","# 5. Generate a batch and verify correctness\n","\n","\n","# 6. Add an augmentation for random horizontal flip (both image & GT)\n","#  Verify by visualization"],"metadata":{"id":"b_ycI6pJrS6V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"3NgNmAEIqnqw"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","import json\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","class CityPersons(Dataset):\n","    def __init__(self, root_dir, split):\n","        assert split in ['train','val','test']\n","        self.root_dir = root_dir\n","        self.split = split\n","        img_tag, ann_tag = \"leftImg8bit\", \"gtBboxCityPersons\"\n","        self.images = sorted(list((Path(root_dir)/\"leftImg8bit\"/split).rglob(\"*.png\")))\n","        self.annotations =[Path(root_dir)/ann_tag/split/path.parent.name/\n","                           path.name.replace(f\"_{img_tag}.png\",f\"_{ann_tag}.json\")\n","                           for path in self.images]\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Get the image\n","        img = Image.open(self.images[idx])\n","        img = transforms.ToTensor()(img)\n","        # Get the GT\n","        bbox_coords = torch.empty(0,4)\n","        bbox_lbl = []\n","        if self.split!='test':\n","            with open(self.annotations[idx]) as f:\n","                anns = json.load(f)\n","            if anns['objects']:\n","                bbox_coords = torch.stack([torch.FloatTensor(ann['bboxVis']) for ann in anns['objects']])\n","                bbox_lbl = [ann['label'] for ann in anns['objects']]\n","        sample = {\n","            'image': img,\n","            'bbox_coords': bbox_coords,\n","            'bbox_lbl': bbox_lbl\n","        }\n","        return sample\n","        #return img, bbox_coords, bbox_lbl\n","\n","    def show(self,idx):\n","        sample = self[idx]\n","        fig, ax = plt.subplots()\n","        ax.imshow(sample['image'].permute((1,2,0)).numpy())\n","        for bbox, lbl in zip(sample['bbox_coords'],sample['bbox_lbl']):\n","            rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=1, edgecolor='r', facecolor='none')\n","            ax.add_patch(rect)\n","            plt.text(bbox[0], bbox[1]-5, lbl,color='yellow')\n","        plt.title(f'CityPersons annotations for \"{self.images[idx].relative_to(self.root_dir)}\"')\n","        plt.show()\n","\n","def my_collate(samples_list):\n","    # Assumes samples_list is a list of dicts; All dicts must have same keys\n","    batch = {}\n","    for key, val in samples_list[0].items():\n","        list_of_vals = [sample[key] for sample in samples_list]\n","        if torch.is_tensor(val) and val.ndim==3:\n","            batch[key] = torch.stack(list_of_vals)\n","        else:\n","            batch[key] = list_of_vals\n","    return batch\n","\n","root_dir = '' # add path cityscapes \n","dataset = CityPersons(root_dir, 'val')\n","sample = dataset[344]\n","\n","dataloader = DataLoader(dataset, batch_size=4, num_workers=0, shuffle=True,\n","                        collate_fn=my_collate)\n","\n","d_iter = iter(dataloader)\n","batch = d_iter.next()\n","\n","for batch in dataloader:\n","    print(batch)"],"metadata":{"id":"a-PDfN8wq1hm"},"execution_count":null,"outputs":[]}]}