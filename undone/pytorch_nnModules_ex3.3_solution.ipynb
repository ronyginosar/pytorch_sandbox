{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmeSF9UesS5cYoYouZn8Q+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"clean file & gitignore again\"\n","!git config --global user.email \"ronyginosar@mail.huji.ac.il\"\n","!git config --global user.name \"ronyginosar\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Empty"],"metadata":{"id":"tlD5OgeAoBts"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import math\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Implements Multi-head Attention Transformer module\n","    This class support both:\n","      - Self-Attention (used in both Tramsformer Encoder and Decoder modules)\n","      - Encoder-Decoder Attention (used only in Decoder modules)\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    Inputs:\n","        (1) x: Tensor of shape (B, Nq, C), for Q projection, where:\n","                * B is the batch size\n","                * Nq is the sequence length (i.e. number of words)\n","                * C is the input dimension (i.e. number of features per word)\n","\n","        (2) [optional] x_kv: Tensor of shape (B, Nkv, C), for K & V projection, where:\n","                * B is the batch size\n","                * Nkv is the sequence length\n","                * C is the input dimension\n","            Used for Encoder-Decoder attention.\n","            If not specified, it is assumed that x_kv=x (i.e. x is used for Q,K,V)\n","\n","        (3) [optional] Attention mask: Boolean Tensor of shape (Nq, Nkv),\n","            indicating which positions each query word can attend to (1 = attend, 0 = do not attend)\n","            For example, this input is used for self-attention mask (Nq,Nq) in decoder layer, allowing the self-attention\n","            to attend only to earlier positions in the output sequence. This is done by masking future positions\n","            (setting them to -inf) before the softmax step in the self-attention calculation.\n","\n","    Output:\n","        output Tensor of shape (B, Nq, C) (similar as above).\n","\n","    Args:\n","        dim (int): Input dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","             Usually (bot not necessarily), dim_head = dim / num_heads\n","\n","    \"\"\"\n","\n","    def __init__(self, ):\n","        ###### Implement Here ######\n","        pass\n","\n","    def forward(self,):\n","        ###### Implement Here ######\n","        pass\n","\n","\n","class EncoderBlock(nn.Module):\n","    \"\"\"\n","    Implements the Encoder model of Transformer architecture:\n","    Multihead Self Attention --> Add & Normalize --> FF --> Add & Normalize\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    Input:\n","        x: Tensor of shape (B, N, C),  where:\n","            * B is the batch size\n","            * N is the sequence length (i.e. number of words)\n","            * C is the input dimension (i.e. number of features per word)\n","\n","       src_mask [optional]: Self-Attention mask; Boolean Tensor of shape (N, N) ;\n","            positions with False are not allowed to attend while True values will be unchanged.\n","\n","    Output:\n","        Tensor of shape (B, N, C) (similar as above),\n","\n","    Args:\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","\n","    \"\"\"\n","    def __init__(self, ):\n","        ###### Implement Here ######\n","        pass\n","\n","    def forward(self,):\n","        ###### Implement Here ######\n","        pass\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Implements a stack of N Encoder Blocks (running sequentially)\n","    Input:\n","        x: Tensor of shape (B, N, C),  where:\n","            * B is the batch size\n","            * N is the sequence length (i.e. number of words)\n","            * C is the input dimension (i.e. number of features per word)\n","\n","        src_mask [optional]: Self-Attention mask Boolean Tensor of shape (N, N) ;\n","            positions with False are not allowed to attend while True values will be unchanged.\n","    Output:\n","        Tensor of shape (B, N, C) - output of the top-most encoder\n","\n","    Args:\n","        num_blocks (int): Number of encoder blocks\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","    \"\"\"\n","    def __init__(self, ):\n","        ###### Implement Here ######\n","        pass\n","\n","    def forward(self,):\n","        ###### Implement Here ######\n","        pass\n","\n","\n","class DecoderBlock(nn.Module):\n","    \"\"\"\n","    Implements a Decoder block of Transformer architecture:\n","    Multihead Self Attention --> Add & Normalize\n","          --> Multihead Encoder-Decoder Attention --> Add & Normalize\n","          --> FF --> Add & Normalize\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    Inputs:\n","            (1) x: Tensor of shape (B, Nq, C), where:\n","                 * B is the batch size\n","                 * Nq is the decoder sequence length (i.e. number of words)\n","                 * C is the input dimension (i.e. number of features per word)\n","\n","            (2) x_kv: Tensor of shape (B, Nkv, C) (similar as above), for K & V projection\n","                (use only for Encoder-Decoder Attention)\n","\n","            (3) [optional] tgt_mask:  Target Attention mask Tensor of shape (Nq, Nq),\n","                used for masking subsequent words during decoding. Applies only for the Self-Attention part.\n","                positions with False are not allowed to attend while True values will be unchanged.\n","            (4) [optional] memory_mask:  Attention mask for encoder-decoder attention (AKA memory)\n","                Tensor of shape (Nq, Nkv).\n","\n","    Output:\n","        Tensor of shape (B, Nq, C)\n","\n","    Args:\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","\n","    \"\"\"\n","\n","    def __init__(self, ):\n","        ###### Implement Here ######\n","        pass\n","\n","    def forward(self,):\n","        ###### Implement Here ######\n","        pass\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Implements a stack of N Decoder Blocks (running sequentially)\n","\n","    Inputs:\n","        x: Tensor of shape (B, Nq, C) for Q projection, where:\n","             * B is the batch size\n","             * N is the sequence length (i.e. number of words)\n","             * C is the input dimension (i.e. number of features per word)\n","        x_kv: Tensor of shape (B, Nkv, C) (similar as above), for K & V projection\n","              (used for encoder-decoder attention modules)\n","        tgt_mask [optional]: Target Attention mask Tensor of shape (Nq, Nq),\n","                used for masking subsequent words during decoding. Applies only for the Self-Attention part.\n","                positions with False are not allowed to attend while True values will be unchanged.\n","        memory_mask [optional]:  Attention mask for encoder-decoder attention (AKA memory),\n","                Tensor of shape (Nq, Nkv).\n","\n","    Output:\n","        Tensor of shape (B, Nq, C) (similar as above),\n","\n","    Args:\n","        num_blocks (int): Number of encoder blocks\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","    \"\"\"\n","    def __init__(self, ):\n","        ###### Implement Here ######\n","        pass\n","\n","    def forward(self,):\n","        ###### Implement Here ######\n","        pass\n","\n","    def generate_square_subsequent_mask(self, ):\n","        ###### Implement Here ######\n","        pass\n","\n","\n","class TokenEmbedding(nn.Module):\n","    \"\"\"\n","    Convert tensor of input indices into corresponding tensor of token embeddings\n","\n","    Input:\n","        input sequence Tensor (B, N) containing indices of vocabulary\n","    Output:\n","        output sequence embedding Tensor (B, N, C)\n","\n","    Args:\n","        vocab_size (int): Vocabulary size\n","        embedding_size (int): embedding dimension (C above)\n","\n","    \"\"\"\n","    def __init__(self,):\n","        ###### Implement Here ######\n","        pass\n","\n","    def forward(self, x):\n","        ###### Implement Here ######\n","        pass\n","\n","class Transformer(nn.Module):\n","    \"\"\"\n","    Implements the Transformer architecture:\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    'forward' function:\n","        Inputs:\n","            (1) Source sequence Tensor of shape (B, Ns), where:\n","                * B is the batch size\n","                * Ns is the source sequence length (i.e. number of words)\n","            (2) Target sequence Tensor of shape (B, Nt), where:\n","                * B is the batch size\n","                * Nt is the target sequence length\n","\n","        Outputs:\n","            Tensor of shape (B, Nt, Vt) with predicted output sequence logits, where\n","                * B is the batch size\n","                * Nt is the target sequence length\n","                * Vt is the target vocabulary size\n","\n","    Args:\n","        src_vocab_size (int): vocabulary size of source sequence\n","        tgt_vocab_size (int): vocabulary size of target sequence\n","        num_blocks (int): Number of encoder blocks\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","    \"\"\"\n","\n","    def __init__(self,):\n","        ###### Implement Here ######\n","        pass\n","\n","    def forward(self, ):\n","        ###### Implement Here ######\n","        pass\n","\n","    @staticmethod\n","    def positional_encoding(input):\n","        \"\"\"\n","        Input:\n","            Tensor of shape (B, N, D), where:\n","                 * B is the batch size\n","                 * N is the sequence length (i.e. number of words)\n","                 * D is the input dimension (i.e. number of features per word)\n","        Output:\n","           Positional Encoding tensor of shape (N, D):\n","\n","        Recommended reference:\n","            https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n","        \"\"\"\n","\n","        ###### Implement Here ######\n","        pass\n","\n","    def greedy_decoding(self,):\n","        \"\"\"\n","        Generate an output sequence given an input sequence, using the greedy iterative algorithm.\n","        It generates an output sequence (y1, ..., ym) of symbols one element at a time.\n","        At each step the model is auto-regressive, consuming the previously generated symbols as\n","        additional input when generating the next. (see https://arxiv.org/pdf/1706.03762.pdf)\n","\n","        Inputs:\n","        :param src_seq: Input test sequence (1,Ns) symbols from source vocabulary\n","        :param start_symbol: symbol indicating start-of-sentence\n","        :param end_symbol: symbol indicating end-of-sentence\n","        :param max_len: maximal output sequence length\n","        :return:\n","            Output sequence (1,Nt) of symbols from target vocabulary\n","        \"\"\"\n","\n","        ###### Implement Here ######\n","        pass\n"],"metadata":{"id":"5wKOCYI-oA-W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"HYiBPBTuoEEr"}},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import math\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Implements Multi-head Attention Transformer module\n","    This class support both:\n","      - Self-Attention (used in both Tramsformer Encoder and Decoder modules)\n","      - Encoder-Decoder Attention (used only in Decoder modules)\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    Inputs:\n","        (1) x: Tensor of shape (B, Nq, C), for Q projection, where:\n","                * B is the batch size\n","                * Nq is the sequence length (i.e. number of words)\n","                * C is the input dimension (i.e. number of features per word)\n","\n","        (2) [optional] x_kv: Tensor of shape (B, Nkv, C), for K & V projection, where:\n","                * B is the batch size\n","                * Nkv is the sequence length\n","                * C is the input dimension\n","            Used for Encoder-Decoder attention.\n","            If not specified, it is assumed that x_kv=x (i.e. x is used for Q,K,V)\n","\n","        (3) [optional] Attention mask: Boolean Tensor of shape (Nq, Nkv),\n","            indicating which positions each query word can attend to (1 = attend, 0 = do not attend)\n","            For example, this input is used for self-attention mask (Nq,Nq) in decoder layer, allowing the self-attention\n","            to attend only to earlier positions in the output sequence. This is done by masking future positions\n","            (setting them to -inf) before the softmax step in the self-attention calculation.\n","\n","    Output:\n","        output Tensor of shape (B, Nq, C) (similar as above).\n","\n","    Args:\n","        dim (int): Input dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","             Usually (bot not necessarily), dim_head = dim / num_heads\n","\n","    \"\"\"\n","    def __init__(self, dim, dim_head, num_heads):\n","        super().__init__()\n","        inner_dim = dim_head * num_heads\n","        self.dim = dim\n","        self.dim_head = dim_head\n","        self.num_heads = num_heads\n","\n","        self.Wk = nn.Linear(dim, inner_dim)\n","        self.Wq = nn.Linear(dim, inner_dim)\n","        self.Wv = nn.Linear(dim, inner_dim)\n","\n","        self.Wo = nn.Linear(inner_dim, dim)\n","\n","\n","    def forward(self,x, x_kv=None, mask=None):\n","        B, N, C = x.shape\n","        assert C==self.dim, \"dimension mismatch\"\n","        if x_kv is not None:\n","            # Suitable for Encoder-Decoder Attention\n","            B1, N1, C1 = x_kv.shape\n","            assert B1 == B and C1 == C, \"both inputs must have identical B,C\"\n","        else:\n","            # Suitable for Self-Attention\n","            x_kv = x\n","\n","        # Calculate K, Q, V:\n","        # B,N,nH*d -> B,N,nH,d ->  B,nH,N,d\n","        q = self.Wq(x).reshape(B, -1, self.num_heads, self.dim_head).permute(0, 2, 1, 3)  # B,H,Nq,d\n","        k = self.Wk(x_kv).reshape(B,-1,self.num_heads, self.dim_head).permute(0,2,1,3)      # B,H,Nkv,d\n","        v = self.Wv(x_kv).reshape(B, -1, self.num_heads, self.dim_head).permute(0, 2, 1, 3) # B,H,Nkv,d\n","\n","        # Calculate Attention\n","        attn = q @ k.transpose(-1,-2) / math.sqrt(self.dim_head) # B,H,Nq,Nkv\n","        if mask is not None:\n","            if isinstance(mask, torch.BoolTensor):\n","                mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n","            attn = attn + mask\n","        attn = torch.softmax(attn,dim=-1) @ v  # B,H,Nq,d\n","\n","        # Concatenate all the attention heads\n","        # B,H,N,d -> B,N,H,d -> B,N,H*d\n","        y = attn.permute(0,2,1,3).reshape(B,N,-1)  # B,Nq,H*d\n","        y = self.Wo(y)\n","        return y\n","\n","\n","class EncoderBlock(nn.Module):\n","    \"\"\"\n","    Implements the Encoder model of Transformer architecture:\n","    Multihead Self Attention --> Add & Normalize --> FF --> Add & Normalize\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    Input:\n","        x: Tensor of shape (B, N, C),  where:\n","            * B is the batch size\n","            * N is the sequence length (i.e. number of words)\n","            * C is the input dimension (i.e. number of features per word)\n","\n","       src_mask [optional]: Self-Attention mask; Boolean Tensor of shape (N, N) ;\n","            positions with False are not allowed to attend while True values will be unchanged.\n","\n","    Output:\n","        Tensor of shape (B, N, C) (similar as above),\n","\n","    Args:\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","\n","    \"\"\"\n","    def __init__(self, dim=512, dim_head=64, num_heads=8, hidden_dim=2048):\n","        super().__init__()\n","        self.dim = dim\n","        self.attention = MultiHeadAttention(dim, dim_head, num_heads)\n","        self.FF = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, dim)\n","        )\n","        self.norm = nn.LayerNorm(dim)\n","\n","    def forward(self, x, src_mask=None):\n","        B, N, C = x.shape\n","        assert C==self.dim, \"dimension mismatch\"\n","        # MSA --> Add & Normalize\n","        x = self.norm(x + self.attention(x, None, src_mask))\n","        # FF --> Add & Normalize\n","        x = x + self.norm(x + self.FF(x))\n","        return x\n","\n","class Encoder(nn.Module):\n","    \"\"\"\n","    Implements a stack of N Encoder Blocks (running sequentially)\n","    Input:\n","        x: Tensor of shape (B, N, C),  where:\n","            * B is the batch size\n","            * N is the sequence length (i.e. number of words)\n","            * C is the input dimension (i.e. number of features per word)\n","\n","        src_mask [optional]: Self-Attention mask Boolean Tensor of shape (N, N) ;\n","            positions with False are not allowed to attend while True values will be unchanged.\n","    Output:\n","        Tensor of shape (B, N, C) - output of the top-most encoder\n","\n","    Args:\n","        num_blocks (int): Number of encoder blocks\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","    \"\"\"\n","    def __init__(self, num_blocks=6, dim=512, dim_head=64, num_heads=8, hidden_dim=2048):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([EncoderBlock(dim, dim_head, num_heads, hidden_dim)\n","                                     for _ in range(num_blocks)])\n","    def forward(self, x, src_mask=None):\n","        for block in self.blocks:\n","            x = block(x, src_mask=src_mask)\n","        return x\n","\n","\n","class DecoderBlock(nn.Module):\n","    \"\"\"\n","    Implements a Decoder block of Transformer architecture:\n","    Multihead Self Attention --> Add & Normalize\n","          --> Multihead Encoder-Decoder Attention --> Add & Normalize\n","          --> FF --> Add & Normalize\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    Inputs:\n","            (1) x: Tensor of shape (B, Nq, C), where:\n","                 * B is the batch size\n","                 * Nq is the decoder sequence length (i.e. number of words)\n","                 * C is the input dimension (i.e. number of features per word)\n","\n","            (2) x_kv: Tensor of shape (B, Nkv, C) (similar as above), for K & V projection\n","                (use only for Encoder-Decoder Attention)\n","\n","            (3) [optional] tgt_mask:  Target Attention mask Tensor of shape (Nq, Nq),\n","                used for masking subsequent words during decoding. Applies only for the Self-Attention part.\n","                positions with False are not allowed to attend while True values will be unchanged.\n","            (4) [optional] memory_mask:  Attention mask for encoder-decoder attention (AKA memory)\n","                Tensor of shape (Nq, Nkv).\n","\n","    Output:\n","        Tensor of shape (B, Nq, C)\n","\n","    Args:\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","\n","    \"\"\"\n","\n","    def __init__(self, dim=512, dim_head=64, num_heads=8, hidden_dim=2048):\n","        super().__init__()\n","        self.dim = dim\n","        self.attention = MultiHeadAttention(dim, dim_head, num_heads)\n","        self.FF = nn.Sequential(\n","            nn.Linear(dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, dim)\n","        )\n","        self.norm = nn.LayerNorm(dim)\n","\n","    def forward(self, x, x_kv, tgt_mask=None, memory_mask=None):\n","        B, N, C = x.shape\n","        B1, N1, C1 = x_kv.shape\n","        assert B1 == B and C1==C, \"both inputs must have identical B,C\"\n","        assert C == self.dim, \"dimension mismatch\"\n","        # MSA --> Add & Normalize\n","        x = self.norm(x + self.attention(x, None, tgt_mask))\n","\n","        # Multihead Encoder-Decoder Attention --> Add & Normalize\n","        x = self.norm(x + self.attention(x, x_kv, memory_mask))\n","\n","        # FF --> Add & Normalize\n","        x = x + self.norm(x + self.FF(x))\n","\n","        return x\n","\n","class Decoder(nn.Module):\n","    \"\"\"\n","    Implements a stack of N Decoder Blocks (running sequentially)\n","\n","    Inputs:\n","        x: Tensor of shape (B, Nq, C) for Q projection, where:\n","             * B is the batch size\n","             * N is the sequence length (i.e. number of words)\n","             * C is the input dimension (i.e. number of features per word)\n","        x_kv: Tensor of shape (B, Nkv, C) (similar as above), for K & V projection\n","              (used for encoder-decoder attention modules)\n","        tgt_mask [optional]: Target Attention mask Tensor of shape (Nq, Nq),\n","                used for masking subsequent words during decoding. Applies only for the Self-Attention part.\n","                positions with False are not allowed to attend while True values will be unchanged.\n","        memory_mask [optional]:  Attention mask for encoder-decoder attention (AKA memory),\n","                Tensor of shape (Nq, Nkv).\n","\n","    Output:\n","        Tensor of shape (B, Nq, C) (similar as above),\n","\n","    Args:\n","        num_blocks (int): Number of encoder blocks\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","    \"\"\"\n","    def __init__(self, num_blocks=6, dim=512, dim_head=64, num_heads=8, hidden_dim=2048):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([DecoderBlock(dim, dim_head, num_heads, hidden_dim)\n","                                     for _ in range(num_blocks)])\n","\n","    def forward(self, x, x_kv, tgt_mask=None, memory_mask=None):\n","        tgt_subsequent_mask = self.generate_square_subsequent_mask(x)\n","        if tgt_mask is not None:\n","            tgt_mask = torch.logical_and(tgt_mask, tgt_subsequent_mask)\n","\n","        for block in self.blocks:\n","            x = block(x, x_kv, tgt_mask, memory_mask)\n","        return x\n","\n","    def generate_square_subsequent_mask(self, input_seq):\n","        # input_seq: B x Nt x C\n","        dim = input_seq.shape[1]\n","        mask = input_seq.new_ones((dim,dim)).tril().bool()\n","        return mask\n","\n","\n","class TokenEmbedding(nn.Module):\n","    \"\"\"\n","    Convert tensor of input indices into corresponding tensor of token embeddings\n","\n","    Input:\n","        input sequence Tensor (B, N) containing indices of vocabulary\n","    Output:\n","        output sequence embedding Tensor (B, N, C)\n","\n","    Args:\n","        vocab_size (int): Vocabulary size\n","        embedding_size (int): embedding dimension (C above)\n","\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_size):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_size)\n","        self.embedding_size = embedding_size\n","\n","    def forward(self, x):\n","        return self.embedding(x.long()) * math.sqrt(self.embedding_size)\n","\n","class Transformer(nn.Module):\n","    \"\"\"\n","    Implements the Transformer architecture:\n","\n","    Reference:\n","        * https://jalammar.github.io/illustrated-transformer/\n","        * https://arxiv.org/pdf/1706.03762.pdf\n","\n","    'forward' function:\n","        Inputs:\n","            (1) Source sequence Tensor of shape (B, Ns), where:\n","                * B is the batch size\n","                * Ns is the source sequence length (i.e. number of words)\n","            (2) Target sequence Tensor of shape (B, Nt), where:\n","                * B is the batch size\n","                * Nt is the target sequence length\n","\n","        Outputs:\n","            Tensor of shape (B, Nt, Vt) with predicted output sequence logits, where\n","                * B is the batch size\n","                * Nt is the target sequence length\n","                * Vt is the target vocabulary size\n","\n","    Args:\n","        src_vocab_size (int): vocabulary size of source sequence\n","        tgt_vocab_size (int): vocabulary size of target sequence\n","        num_blocks (int): Number of encoder blocks\n","        dim (int): Input/Output dimension (#channels)\n","        num_heads (int): Number of heads\n","        dim_head (int): Per-head dimention\n","        hidden_dim (int): dimension of FF hidden layer\n","    \"\"\"\n","\n","    def __init__(self, src_vocab_size=1000, tgt_vocab_size=1000,\n","                 num_layers=6, dim=512, dim_head=64, num_heads=8, hidden_dim=2048):\n","        super().__init__()\n","        self.encoder = Encoder(num_layers, dim, dim_head, num_heads, hidden_dim)\n","        self.decoder = Decoder(num_layers, dim, dim_head, num_heads, hidden_dim)\n","\n","        self.src_embedding = TokenEmbedding(src_vocab_size, dim)\n","        self.tgt_embedding = TokenEmbedding(tgt_vocab_size, dim)\n","\n","        self.prediction_head = nn.Linear(dim,tgt_vocab_size)\n","\n","    @property\n","    def device(self):\n","        p=next(iter(self.parameters()))\n","        return p.device\n","\n","    def forward(self, src_seq, tgt_seq):\n","        Ns, Nt = src_seq.shape[-1], tgt_seq.shape[-1]\n","\n","        # Linear Embedding\n","        src_emb = self.src_embedding(src_seq) # B,Ns,dim\n","        tgt_emb = self.tgt_embedding(tgt_seq) # B,Nt,dim\n","\n","        # Positional Encoding\n","        src_emb = self.positional_encoding(src_emb)\n","        tgt_emb = self.positional_encoding(tgt_emb)\n","\n","        # Encoder --> Decoder\n","        enc_out = self.encoder(src_emb)   # B,Ns,dim\n","        dec_out = self.decoder(tgt_emb, enc_out) # B,Nt,dim\n","\n","        # Final prediction head\n","        logits = self.prediction_head(dec_out) # B,Nt,tgt_vocab_size\n","\n","        return logits\n","\n","    @staticmethod\n","    def positional_encoding(input):\n","        \"\"\"\n","        Input:\n","            Tensor of shape (B, N, D), where:\n","                 * B is the batch size\n","                 * N is the sequence length (i.e. number of words)\n","                 * D is the input dimension (i.e. number of features per word)\n","        Output:\n","           Positional Encoding tensor of shape (N, D):\n","\n","        Recommended reference:\n","            https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n","        \"\"\"\n","        b, n, d = input.shape\n","        omega = (1 / 10000 ** (torch.arange(0, d, 2) / d)).repeat_interleave(2)\n","        t = torch.arange(n)\n","        pos_enc = t[:, None] * omega[None, :]\n","        pos_enc[:, 0::2] = torch.sin(pos_enc[:, 0::2])\n","        pos_enc[:, 1::2] = torch.cos(pos_enc[:, 1::2])\n","        return input + pos_enc\n","\n","    def greedy_decoding(self, src_seq, start_symbol=0, end_symbol=1, max_len=50):\n","        \"\"\"\n","        Generate an output sequence given an input sequence, using the greedy iterative algorithm.\n","        It generates an output sequence (y1, ..., ym) of symbols one element at a time.\n","        At each step the model is auto-regressive, consuming the previously generated symbols as\n","        additional input when generating the next. (see https://arxiv.org/pdf/1706.03762.pdf)\n","\n","        Inputs:\n","        :param src_seq: Input test sequence (1,Ns) symbols from source vocabulary\n","        :param start_symbol: symbol indicating start-of-sentence\n","        :param end_symbol: symbol indicating end-of-sentence\n","        :param max_len: maximal output sequence length\n","        :return:\n","            Output sequence (1,Nt) of symbols from target vocabulary\n","        \"\"\"\n","\n","        # Linear Embedding + Positional Encoding\n","        src_emb = self.positional_encoding(self.src_embedding(src_seq))\n","\n","        memory = self.encoder(src_emb)   # 1,Ns,dim\n","\n","        tgt_seq = src_seq.new_tensor(start_symbol).view(1, 1)\n","        for i in range(max_len-1):\n","            tgt_emb = self.positional_encoding(self.tgt_embedding(tgt_seq))\n","            dec_out = self.decoder(tgt_emb, memory) # B,Nt,dim\n","            logits = self.prediction_head(dec_out[:, -1:, :])\n","            next_word = torch.argmax(logits, dim=-1)\n","            tgt_seq = torch.cat((tgt_seq, next_word), dim=1)\n","            if next_word.item()==end_symbol:\n","                break\n","\n","        return tgt_seq\n","\n","src_vocab_size=1000\n","tgt_vocab_size=1000\n","batch_size = 4\n","Ns = 20\n","Nt = 10\n","src = torch.randint(0,src_vocab_size, (batch_size, Ns))\n","tgt = torch.randint(0,tgt_vocab_size, (batch_size, Nt))\n","seq2seq = Transformer(src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size)\n","out = seq2seq(src, tgt)\n","\n","src = torch.randint(0,src_vocab_size, (1, Ns))\n","out_greedy = seq2seq.greedy_decoding(src)\n","\n","\n"],"metadata":{"id":"2FpHbDI_n5Ti"},"execution_count":null,"outputs":[]}]}