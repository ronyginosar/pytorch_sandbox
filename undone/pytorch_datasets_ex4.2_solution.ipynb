{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOW9eIILg5abe56uQCBJJQp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"clean file & gitignore again\"\n","!git config --global user.email \"ronyginosar@mail.huji.ac.il\"\n","!git config --global user.name \"ronyginosar\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Empty"],"metadata":{"id":"jrOWrHBArL7g"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","from PIL import Image\n","import torchvision.transforms as transforms\n","\n","dataset_root = ''# add path cityscapes\n","\n","# 1. Implement the custom Dataset\n","class Cityscapes(Dataset):\n","    def __init__(self, dataset_root, split, ...):\n","        assert split in ['train','val','test']\n","        ...\n","\n","    def __len__(self):\n","        return ...\n","\n","    def __getitem__(self, idx):\n","        ...\n","        return img, seg\n","\n","    # visualize a sample of the dataset (index idx)\n","    def show(self, idx):\n","        ...\n","\n","# 2. Create an instance of the dataset and verify by visualization\n","dataset = Cityscapes(...)\n","\n","# 3. Add data augmentation (to both input & GT) and verify\n","# Resize to (256,512)  -> Random Crop (256,256) -> Random Horizontal Flip\n","# hint – what type of interpolation to use for resizing?\n","# Data & GT augmentations must be matched, so write custom ‘pairwise’ transforms\n","# Hint – For Resize, use an existing functional transform: transforms.functional.resize\n","# Hint – For random crop, use transforms.RandomCrop.get_params to get the crop indices\n","# Use transforms.Compose on the custom augmentations\n","\n","# For example:\n","class RandomHorizontalFlip_pairwise:\n","    def __init__(self, p=0.5):\n","        super().__init__()\n","        ...\n","\n","    def __call__(self, img, mask):\n","        ...\n","        transformed_img = ...\n","        transformed_mask = ...\n","        return transformed_img, transformed_mask\n","\n","\n","# 4. Create a dataloader, generate batches and check validity (+ visualize!)\n","\n","\n","\n","\n"],"metadata":{"id":"BylkkgbCrKa3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"s9uN2zNPrNwq"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from pathlib import Path\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","root_dir = ''# add path cityscapes\n","\n","class Cityscapes(Dataset):\n","    def __init__(self, root_dir, split, pairwise_transform=None):\n","        assert split in ['train','val','test']\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.images = sorted(list((Path(root_dir)/\"leftImg8bit\"/split).rglob(\"*.png\")))\n","        self.labelIds =[Path(root_dir)/\"gtFine\"/split/path.parent.name/\n","                        path.name.replace(\"_leftImg8bit.png\",f\"_gtFine_labelIds.png\")\n","                        for path in self.images]\n","        self.pairwise_transform = pairwise_transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    @staticmethod\n","    def read_seg_file(file):\n","        segm = Image.open(file)\n","        segm = torch.from_numpy(np.array(segm, np.uint8))\n","        segm = segm.unsqueeze(0)\n","        return segm\n","\n","    def __getitem__(self, idx):\n","        # Get the image\n","        img = transforms.ToTensor()(Image.open(self.images[idx]))\n","        # Get the GT\n","        segm = self.read_seg_file(self.labelIds[idx])\n","        # Apply pairwise transform\n","        if self.pairwise_transform is not None:\n","            img, segm = self.pairwise_transform((img, segm))\n","\n","        return img, segm\n","\n","    def show(self, idx):\n","        img, segm = self[idx]\n","        fig, (ax_im, ax_seg) = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(16,4))\n","        ax_im.imshow(img.permute(1,2,0).cpu().numpy())\n","        ax_im.set(title=\"image\")\n","        ax_seg.imshow(segm.permute(1,2,0).cpu().numpy())\n","        ax_seg.set(title=\"GT segmentation\")\n","\n","class PairwiseRandomHorizontalFlip:\n","    def __init__(self, p=0.5):\n","        self.p = p\n","\n","    def __call__(self, sample):\n","        img, target = sample\n","        if torch.rand(1) < self.p:\n","            return transforms.functional.hflip(img), transforms.functional.hflip(target)\n","        return sample\n","\n","class PairwiseResize:\n","    def __init__(self, output_size,\n","                     interp_img=transforms.InterpolationMode.BILINEAR,\n","                     interp_tgt=transforms.InterpolationMode.NEAREST):\n","        self.resize_image = transforms.Resize(output_size, interpolation=interp_img)\n","        self.resize_target = transforms.Resize(output_size, interpolation=interp_tgt)\n","\n","    def __call__(self, sample):\n","        assert isinstance(sample, tuple) and len(sample)==2\n","        assert sample[0].shape[-2:] == sample[1].shape[-2:]\n","        img, target = sample\n","        return self.resize_image(img), self.resize_target(target)\n","\n","\n","class PairwiseRandomCrop:\n","    def __init__(self, output_size):\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        assert isinstance(sample, tuple) and len(sample) == 2\n","        assert sample[0].shape[-2:] == sample[1].shape[-2:]\n","        img, target = sample\n","        # randomize cropping position\n","        crop_params = transforms.RandomCrop.get_params(img, self.output_size)\n","        img_crop = transforms.functional.crop(img, *crop_params)\n","        target_crop = transforms.functional.crop(target, *crop_params)\n","        return img_crop, target_crop\n","\n","transform = transforms.Compose([\n","    PairwiseResize((512,1024)),\n","    PairwiseRandomCrop((256,256)),\n","    PairwiseRandomHorizontalFlip()\n","])\n","\n","dataset = Cityscapes(root_dir, 'train', pairwise_transform=transform)\n","dataset.show(0)\n","\n","dataloader = DataLoader(dataset, batch_size=8)\n","\n","img_batch, gt_batch = iter(dataloader).next()\n","print(img_batch.shape)\n","print(gt_batch.shape)\n","a=1\n","\n","\n"],"metadata":{"id":"qde-lEAcq_uT"},"execution_count":null,"outputs":[]}]}