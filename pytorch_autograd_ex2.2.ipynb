{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrcapi4G4a1ZAHIrelsKyZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"clean file & gitignore again\"\n","!git config --global user.email \"ronyginosar@mail.huji.ac.il\"\n","!git config --global user.name \"ronyginosar\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","\n","# %% Implement the following mini NN\n","x = torch.randn(1, 6, requires_grad=True)\n","w = torch.randn(4, 6, requires_grad=True)\n","y = x @ w.t()  # transpose w, y size (1,4)\n","y.retain_grad()  # to later compare with autograd\n","z = F.relu(y)\n","z.retain_grad()\n","t = torch.randn(1, 4, requires_grad=True)  # target for mse\n","L = F.mse_loss(z, t)\n","\n","# %% Implement backpropagation (without autograd!) to find dL/dw\n","# Calculate the gradients analytically, then traverse backward using the chain rule\n","# Go step by step, comparing to Autograd, using w.grad and .retain_grad() and torch.equal\n","L.backward() # with autograd, to compare to step-by-step\n","\n","# %% Start from dL/dz\n","dL_dz = 2/(z.size(0) * z.size(1)) * (z-t)\n","torch.equal(dL_dz, z.grad)\n","\n","# %% then dL/dy = dL/dz * dz/dy\n","dz_dy = ((z > 0)*1 + (z < 0)*0).to(torch.float)  # or (z>0).to(torch.float)\n","dL_dy = dL_dz * dz_dy\n","torch.equal(dL_dy, y.grad)\n","\n","# %% then dL/dw = dL/dy * dy/dw\n","dy_dw = x\n","dL_dw = dL_dy.t() * dy_dw  # the transpose on dL_dy, not on x...\n","torch.equal(dL_dw, w.grad)\n"],"metadata":{"id":"ki_1h3tRmQ2B"},"execution_count":null,"outputs":[]}]}