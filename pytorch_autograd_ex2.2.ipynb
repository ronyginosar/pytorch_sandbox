{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvfGMax+O3PiPV0tlQPdnK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"DO4sLYCWhILM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670062517064,"user_tz":-120,"elapsed":22172,"user":{"displayName":"Rony Ginosar","userId":"05802852399170902908"}},"outputId":"cf951b1b-eb80-4446-c905-72bd747b50f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","mkdir: cannot create directory ‘/content/drive/My Drive/CNNStanford/pytorch’: File exists\n","PROJECT_PATH: /content/drive/My Drive/CNNStanford/pytorch\n","/content/drive/My Drive/CNNStanford/pytorch/pytorch_sandbox\n"]}],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"ex2.2\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implement manual backpropagation without autograd"],"metadata":{"id":"uhIv-w9Pepum"}},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","\n","\n","# %% Implement the following mini NN\n","x = torch.randn(1, 6, requires_grad=True)\n","w = torch.randn(4, 6, requires_grad=True)\n","y = x @ w.t()  # transpose w, y size (1,4)\n","y.retain_grad()  # to later compare with autograd\n","z = F.relu(y)\n","z.retain_grad()\n","t = torch.randn(1, 4, requires_grad=True)  # target for mse\n","L = F.mse_loss(z, t)\n","\n","# %% Implement backpropagation (without autograd!) to find dL/dw\n","# Calculate the gradients analytically, then traverse backward using the chain rule\n","# Go step by step, comparing to Autograd, using w.grad and .retain_grad() and torch.equal\n","L.backward() # with autograd, to compare to step-by-step\n","\n","# %% Start from dL/dz\n","dL_dz = 2/(z.size(0) * z.size(1)) * (z-t)\n","torch.equal(dL_dz, z.grad)\n","\n","# %% then dL/dy = dL/dz * dz/dy\n","dz_dy = ((z > 0)*1 + (z < 0)*0).to(torch.float)  # or (z>0).to(torch.float)\n","dL_dy = dL_dz * dz_dy\n","torch.equal(dL_dy, y.grad)\n","\n","# %% then dL/dw = dL/dy * dy/dw\n","dy_dw = x\n","dL_dw = dL_dy.t() * dy_dw  # the transpose on dL_dy, not on x...\n","torch.equal(dL_dw, w.grad)\n"],"metadata":{"id":"ki_1h3tRmQ2B","colab":{"base_uri":"https://localhost:8080/","height":368},"executionInfo":{"status":"error","timestamp":1670062548629,"user_tz":-120,"elapsed":73,"user":{"displayName":"Rony Ginosar","userId":"05802852399170902908"}},"outputId":"c32930ce-657f-4a1f-c3b1-3a9b37ed6ce3"},"execution_count":5,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-fa215d0ee249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# %% Implement the following mini NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: THPDtypeType.tp_dict == nullptr INTERNAL ASSERT FAILED at \"../torch/csrc/Dtype.cpp\":139, please report a bug to PyTorch. "]}]}]}