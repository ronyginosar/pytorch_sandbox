{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTCXR4IgACgX+5Md9R9Bjb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"clean file & gitignore again\"\n","!git config --global user.email \"ronyginosar@mail.huji.ac.il\"\n","!git config --global user.name \"ronyginosar\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Empty"],"metadata":{"id":"-LHSrisZtnSV"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchmetrics\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import sys\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","from tqdm import tqdm\n","\n","VOC_classes = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n","               'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n","               'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n","\n","def main(argv):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Define the model (see https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n","    # Dont forget to use the pretrained weights (they are downloaded automatically)\n","    model = ...\n","    model.eval()\n","    model = model.to(device)\n","\n","    # Define transform for image\n","    # (A) Convert to tensor\n","    # (B) Normalize with mean & std (see https://pytorch.org/vision/stable/models.html#semantic-segmentation)\n","    transform = transforms.Compose([\n","        ...\n","    ])\n","\n","    # Define transform for target\n","    # (A) Convert to tensor\n","    # (B) Squeeze to 2D label map (HxW)\n","    # (C) convert float labels to integers: multiply by 255 and cast to long int.\n","    # (D) Replace void labels (255) with 21 (i.e. define it as the 22'th class, to be ignored later)\n","    # Hint - use transforms.Lambda for generic transforms\n","    transform_target = transforms.Compose([\n","        ...\n","    ])\n","    # Define the 'VOC 2012 Val' dataset (see  use the pretrained weights (they are downloaded automatically)\n","    # Use this root dir: \"/stage/algo-datasets/DB/PyTorch/VOC\"\n","    dataset = ...\n","\n","    #The dataset GT labels have the folloowing form:\n","    #  0: background\n","    #  [1.. 20]  interval: segmented objects, classes[Aeroplane, ..., Tvmonitor]\n","    #  255: void category, used for border regions (5px) and to mask difficult objects\n","\n","\n","    dataloader = ...\n","\n","    # Define metrics as torchmetrics modules\n","    # (documentation: https://torchmetrics.readthedocs.io/en/v0.6.1/references/modules.html#classification-metrics)\n","    # hints: Use 22 classes and define ignore_index=21\n","    mIOU = ...\n","    acc_global = ...\n","    acc_per_class = ...\n","\n","    # Test Loop\n","    with torch.no_grad():\n","        for i, (images, targets) in enumerate(tqdm(dataloader)):\n","            images = images.to(device)\n","            targets = targets.to(device)\n","            ...\n","            ...\n","\n","    # Print results:\n","    print(f'mIOU = {... :.2f}%')\n","    print(f'global acc = {... :.2f}%')\n","    for i, class_name in enumerate(VOC_classes):\n","        print(f'\"{class_name}\" accuracy = {... :.2f}%')\n","\n","\n","\n","if __name__ == \"__main__\":\n","    main(sys.argv[1:])"],"metadata":{"id":"NYvg2XoUtooS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"xRVbAit4tpcV"}},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torchmetrics\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","import sys\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","from tqdm import tqdm\n","\n","VOC_classes = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n","               'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n","               'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n","\n","def main(argv):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)\n","    model.eval()\n","    model = model.to(device)\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean, std)\n","    ])\n","    transform_target = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.squeeze()),\n","        transforms.Lambda(lambda x: (255*x).long()),\n","        transforms.Lambda(lambda x: torch.where(x==255,21,x))\n","    ])\n","    dataset = torchvision.datasets.VOCSegmentation(root=\"__\",\n","                                                   year=\"2012\", image_set=\"val\", download=False,\n","                                                   transform=transform,\n","                                                   target_transform=transform_target\n","                                                   )\n","    '''\n","    0: background\n","    [1.. 20]  interval: segmented objects, classes[Aeroplane, ..., Tvmonitor]    \n","    255: void category, used for border regions (5px) and to mask difficult objects\n","    '''\n","\n","    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n","\n","    mIOU = torchmetrics.IoU(num_classes=22, ignore_index=21).to(device)\n","    acc_global = torchmetrics.Accuracy(num_classes=22, average='micro',mdmc_average='global', ignore_index=21).to(device)\n","    acc_per_class = torchmetrics.Accuracy(num_classes=22, average='none', mdmc_average='global', ignore_index=21).to(device)\n","\n","    # Test Loop\n","    with torch.no_grad():\n","        for i, (images, targets) in enumerate(tqdm(dataloader)):\n","            images = images.to(device)\n","            targets = targets.to(device)\n","\n","            preds = model(images)['out']\n","            pred_cls = preds.argmax(dim=1)\n","\n","            mIOU.update(pred_cls,targets)\n","            acc_global.update(pred_cls,targets)\n","            acc_per_class.update(pred_cls,targets)\n","\n","    print(f'mIOU = {100*mIOU.compute():.2f}%')\n","    print(f'global acc = {100*acc_global.compute():.2f}%')\n","    acc_per_class = acc_per_class.compute()\n","    for i, class_name in enumerate(VOC_classes):\n","        print(f'\"{class_name}\" accuracy = {100 * acc_per_class[i].item() :.2f}%')\n","\n","\n","if __name__ == \"__main__\":\n","    main(sys.argv[1:])"],"metadata":{"id":"bOlTxNt3tqUZ"},"execution_count":null,"outputs":[]}]}