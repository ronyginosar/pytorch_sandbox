{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6pLFMD6CXAsi0j8twLJZX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"clean file & gitignore again\"\n","!git config --global user.email \"ronyginosar@mail.huji.ac.il\"\n","!git config --global user.name \"ronyginosar\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from pathlib import Path\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","\n","DATA_ROOT = \"\" # add sample DS\n","\n","# (1) Define Dataset\n","class GenericImageFolder(Dataset):\n","    def __init__(self, root_dir):\n","        # Get the root dir of images\n","        self.root_dir = root_dir\n","        # Find for all jpeg images recursively  (e.g. using Pathlib & rglob)\n","        # images = []\n","        # p = Path(root_dir)\n","        # for i in p.rglob('*.jpg'): --> p.glob s already recursive...\n","        #     images.append(i.name)\n","        images = list(Path(root_dir).rglob(\"*.jpg\"))\n","        self.images = images\n","        # used transform\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Resize((256, 256))\n","        ])\n","\n","    def __len__(self):\n","        return len(self.images)  # was missing\n","\n","    def __getitem__(self, idx):\n","        # Loads an image (by idx) from disk\n","        im_path = Path(self.root_dir)/self.images[idx]\n","        image = Image.open(im_path)\n","        # (using transforms) Resize to 256x256 and convert to tensor  (transforms.Resize, ToTensor)\n","        # image = transforms.ToTensor()(image)\n","        # image = transforms.Resize((256, 256))\n","        # moved above to init + compose(sequence of transforms):\n","        image = self.transform(image)\n","        # Return the image as a float tensor 3x256x256 (range [0, 1])   (no GT)  -> built in to ToTransform\n","        return image\n","\n","\n","# a function that gets an image tensor and displays it to screen (using matplotlib)\n","def show_image(im_tensor):\n","    im = transforms.ToPILImage()(im_tensor.to('cpu'))\n","    plt.imshow(np.asarray(im))\n","    # or from previous ex1.2: plt.imshow(im_tensor.permute((1,2,0)).cpu().numpy())\n","    plt.show()  # needed for imshow\n","\n","\n","# (2) Create dataset\n","dataset = GenericImageFolder(DATA_ROOT)\n","\n","# (3) Visualize a dataset sample\n","show_image(dataset[2])\n","\n","# (4) Create dataloader with batch_size=8\n","dataloader = DataLoader(dataset, batch_size=8)\n","# sanity:\n","# print(len(dataset), len(dataloader))\n","\n","# (5) Iterate through dataloader\n","mean = torch.Tensor([0, 0, 0])  # torch.zeros(3)\n","dataloaderIterator = iter(dataloader)  # need to init this outside of loop, otherwise we get the same batch.\n","for batch in dataloader:\n","    img_batch = next(dataloaderIterator)\n","    # sanity:\n","    # print(f\"images: shape={img_batch.shape}, type={img_batch.dtype}\")\n","\n","    # (6) Calculate mean R,G,B values across all images and spatial locations\n","    # shape of images (batch_size, c, h, w)\n","    # c is the depth of the patches (since they are RGB, so c=3)\n","    # no need to break into rgb - it returns a 1*3 tensor that saves them as is\n","    batch_mean = img_batch.shape[0] * img_batch.mean([0, 2, 3])  # note: these dims are the ones reduced\n","    mean += batch_mean\n","    # sanity\n","    # print(batch_mean, img_batch.shape[0])\n","    # why do we have to *batchsize and then later /datasetsize?\n","    # since we mean over each batch and want to the divide by all items.\n","    # however - 1) last batch can be different size (hence can do it outside of loop)\n","    # 2) # of items is no longer datasetsize but some adaptation (smaller), according to how many items in each batch\n","    # batch.mean(dim=(0,2,3)) vs torch.mean(batch, (0,2,3)) - same\n","\n","print(f'mean RGB = {mean / len(dataset)}')\n"],"metadata":{"id":"WKcUhA7IoodI"},"execution_count":null,"outputs":[]}]}