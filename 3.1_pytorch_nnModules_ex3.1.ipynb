{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCEFTBqBBFr0+6aYihJ/Fy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DO4sLYCWhILM"},"outputs":[],"source":["# Mout Google Drive\n","# https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n","from google.colab import drive\n","ROOT = \"/content/drive\"\n","drive.mount(ROOT)\n","# %pwd %ls\n","# run github settings\n","%run /content/drive/MyDrive/CNNStanford/pytorch/pytorch_sandbox/Colab_Helper.ipynb"]},{"cell_type":"code","source":["MESSAGE = \"clean file & gitignore again\"\n","!git config --global user.email \"ronyginosar@mail.huji.ac.il\"\n","!git config --global user.name \"ronyginosar\"\n","!git add ."],"metadata":{"id":"A6ENAdliiQ9c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"{MESSAGE}\"\n","!git push \"{GIT_PATH}\""],"metadata":{"id":"9QWMKhNdiTJ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import sys\n","import numpy as np\n","from torchinfo import summary\n","import torch.nn.functional as F\n","\n","\n","#%% Build a simple classification CNN, using custom Module/s\n","class RB(nn.Module):\n","    \"\"\" initial manual block \"\"\"\n","    def __init__(self, in_channels, out_channels, stride=1, padding=1, kernel=3):\n","        super().__init__()\n","        # one by one\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding)\n","        self.conv1_bn = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding)\n","        self.conv2_bn = nn.BatchNorm2d(out_channels)\n","        self.residual = None\n","        if in_channels != out_channels:\n","            self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.conv1_bn(out)\n","        out = F.relu(out) # can move to module and use nn.relu\n","        out = self.conv2(out)\n","        out = self.conv2_bn(out)\n","        if self.residual is not None:\n","            out += self.residual(x)\n","        else:\n","            out += x\n","        out = F.relu(out)\n","        return out\n","\n","\n","#%%\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, padding=1, kernel=3):\n","        super().__init__()\n","        # sequential\n","        self.blocks = nn.Sequential(conv_block(in_channels, out_channels, kernel, stride, padding),\n","                                    nn.ReLU(),\n","                                    conv_block(out_channels, out_channels, kernel, stride, padding)\n","                                    )\n","        self.residual = None\n","        if in_channels != out_channels:\n","            self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n","\n","    def forward(self, x):\n","        out = self.blocks(x)\n","        if self.residual is not None:\n","            out += self.residual(x)\n","        else:\n","            out += x\n","        out = F.relu(out)\n","        return out\n","\n","\n","def conv_block(in_f, out_f, *args, **kwargs):\n","    return nn.Sequential(nn.Conv2d(in_f, out_f, *args, **kwargs),\n","                         nn.BatchNorm2d(out_f)\n","                         )\n","\n","\n","#%%\n","class RBPool(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, padding=1, kernel=3, pool='max'):\n","        super().__init__()\n","        self.pool = nn.Identity()\n","        if pool == 'max':\n","            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        elif pool == 'avg':\n","            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        self.rb_pool = nn.Sequential(ResidualBlock(in_channels, out_channels, stride=1, padding=1, kernel=3),\n","                                     self.pool\n","                                     )\n","\n","    def forward(self, x):\n","        return self.rb_pool(x)\n","\n","\n","#%%\n","class Ex1Net(nn.Module):\n","    def __init__(self, in_channels, out_channels, pools, num_classes):\n","        super().__init__()\n","        num_layers = len(in_channels)\n","        # using ModuleList\n","        # self.layers = nn.ModuleList([RBPool(in_channels[i], out_channels[i], pools[i]) for i in range(num_layers)])\n","\n","        # using Sequential and zip\n","        layers = [RBPool(in_c, out_c, pool=p)\n","                  for in_c, out_c, p in\n","                  zip(in_channels, out_channels, pools)]\n","        self.layers = nn.Sequential(*layers)\n","\n","        self.linear = nn.Linear(out_channels[-1], num_classes)\n","        self.linear_in_dim = out_channels[-1]\n","\n","    def forward(self, x):\n","        # # for moduleList\n","        # for layer in self.layers:\n","        #     x = layer(x)\n","\n","        # # for sequential\n","        x = self.layers(x)\n","\n","        # Hint: need to reshape before applying FC\n","        x = x.reshape(-1, self.linear_in_dim)  # flat for FC, the size -1 is inferred from other dimensions\n","        x = self.linear(x)\n","        return x\n","\n","#%%\n","def count_params(net):\n","    # 'net' can be any nn.Module\n","    # hint: net.parameters -or- net.named_parameters(), .parameters() is an iterator\n","\n","    # PyTorch torch.numel() method returns the total number of elements in the input tensor.\n","\n","    # trainable parameters (remove if for all params)\n","    # total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n","    total_params = sum(p.numel() for p in net.parameters())\n","\n","    return total_params\n","\n","#%%\n","def main(argv):\n","\n","    # Create a Ex1Net instance\n","    net = Ex1Net(in_channels=[3, 32, 64, 128],\n","                 out_channels=[32, 64, 128, 256],\n","                 pools=['max', 'max', 'max', 'avg'],\n","                 num_classes=5)\n","\n","    # - set the network  to 'eval' mode\n","    net.eval()\n","    # - generate a random input batch [Nx3x32x32]\n","    in_images = 10\n","    x = torch.randn((in_images, 3, 32, 32))\n","\n","    # - feed the batch through the network (forward), using CPU\n","    if not torch.cuda.is_available():\n","        print(\"running on CPU\")\n","        y = net(x)\n","\n","    # - feed the batch through the network (forward), using GPU\n","    #   hint: make sure both input and network are moved to GPU.\n","    if torch.cuda.is_available():\n","        print(\"running on GPU\")\n","        device = torch.device('cuda:0')\n","        x = x.to(device)\n","        net = net.to(device)\n","        y = net(x)\n","\n","    # - Calculate the number of parameters in the network\n","    num_params = count_params(net)\n","    print(f\"Total number of parameters: {num_params}\")\n","\n","    # - Make a nice summary table of the network using 'torchinfo'\n","    # Display the following columns: input size, output size, #params, #MACs per layer\n","    # ex. from torchinfo import summary ; summary(net, input_size=(1,3,224,224),...)\n","    # print needed for notebook mode\n","\n","    print(summary(net,\n","                  input_size=(in_images, 3, 32, 32),\n","                  col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\"],\n","                  col_width=15,\n","                  depth=10))\n","\n","\n","#%%\n","if __name__ == \"__main__\":\n","    main(sys.argv)\n"],"metadata":{"id":"UIpJEXs3ngzG"},"execution_count":null,"outputs":[]}]}