{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFJ+go1tZT/ch5lqJu6ILH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["class RB(nn.Module):\n","    \"\"\" initial manual block \"\"\"\n","    def __init__(self, in_channels, out_channels, stride=1, padding=1, kernel=3):\n","        super().__init__()\n","        # one by one\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding)\n","        self.conv1_bn = nn.BatchNorm2d(out_channels)\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding)\n","        self.conv2_bn = nn.BatchNorm2d(out_channels)\n","        self.residual = None\n","        if in_channels != out_channels:\n","            self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = self.conv1_bn(out)\n","        out = F.relu(out) # can move to module and use nn.relu\n","        out = self.conv2(out)\n","        out = self.conv2_bn(out)\n","        if self.residual is not None:\n","            out += self.residual(x)\n","        else:\n","            out += x\n","        out = F.relu(out)\n","        return out\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, padding=1, kernel=3):\n","        super().__init__()\n","        # sequential\n","        self.blocks = nn.Sequential(conv_block(in_channels, out_channels, kernel, stride, padding),\n","                                    nn.ReLU(),\n","                                    conv_block(out_channels, out_channels, kernel, stride, padding)\n","                                    )\n","        self.residual = None\n","        if in_channels != out_channels:\n","            self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n","\n","    def forward(self, x):\n","        out = self.blocks(x)\n","        if self.residual is not None:\n","            out += self.residual(x)\n","        else:\n","            out += x\n","        out = F.relu(out)\n","        return out\n","\n","\n","def conv_block(in_f, out_f, *args, **kwargs):\n","    return nn.Sequential(nn.Conv2d(in_f, out_f, *args, **kwargs),\n","                         nn.BatchNorm2d(out_f)\n","                         )\n","\n","class RBPool(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, padding=1, kernel=3, pool='max'):\n","        super().__init__()\n","        self.pool = nn.Identity()\n","        if pool == 'max':\n","            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","        elif pool == 'avg':\n","            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        self.rb_pool = nn.Sequential(ResidualBlock(in_channels, out_channels, stride=1, padding=1, kernel=3),\n","                                     self.pool\n","                                     )\n","\n","    def forward(self, x):\n","        return self.rb_pool(x)\n","\n","class Ex1Net(nn.Module):\n","    def __init__(self, in_channels, out_channels, pools, num_classes):\n","        super().__init__()\n","        num_layers = len(in_channels)\n","        # using ModuleList\n","        # self.layers = nn.ModuleList([RBPool(in_channels[i], out_channels[i], pools[i]) for i in range(num_layers)])\n","\n","        # using Sequential and zip\n","        layers = [RBPool(in_c, out_c, pool=p)\n","                  for in_c, out_c, p in\n","                  zip(in_channels, out_channels, pools)]\n","        self.layers = nn.Sequential(*layers)\n","\n","        self.linear = nn.Linear(out_channels[-1], num_classes)\n","        self.linear_in_dim = out_channels[-1]\n","\n","    def forward(self, x):\n","        # # for moduleList\n","        # for layer in self.layers:\n","        #     x = layer(x)\n","\n","        # # for sequential\n","        x = self.layers(x)\n","\n","        # Hint: need to reshape before applying FC\n","        x = x.reshape(-1, self.linear_in_dim)  # flat for FC, the size -1 is inferred from other dimensions\n","        x = self.linear(x)\n","        return x\n","\n","\n"],"metadata":{"id":"UIpJEXs3ngzG"},"execution_count":null,"outputs":[]}]}